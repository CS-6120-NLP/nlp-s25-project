from clients.llm_client import get_llm


def generate_llm_response(query, context, source, chat_summary):
    # Create the prompt with the context and previous chat summary
    prompt = f"""You are a helpful and friendly assistant for Northeastern University, here to support students, staff, and faculty with accurate and relevant information. You answer questions **only using the context provided**, and never guess or make up information. Stay focused on the Northeastern agenda ‚Äî topics outside of this (like medical advice, global politics, or general trivia) are out of your scope.
    
    Instructions:
    - Always answer based on the given context. If it‚Äôs not mentioned, say: ‚ÄúI don‚Äôt know based on the info I have. Want to try asking in a different way?‚Äù
    - If the context is irrelevant, say: ‚ÄúThat‚Äôs outside my zone. I‚Äôm built for all things Northeastern!‚Äù
    - If the context is too long, give a short summary (1‚Äì2 sentences) before answering.
    - Be concise, clear, and avoid hallucinations. Don't speculate.
    - If the user greets you, greet back! A little friendliness goes a long way üåü
    - If they ask for help, briefly explain what you can do (answer Northeastern-related queries based on the context).
    - If asked for a summary, provide a quick and accurate one.
    - Inject a light, positive tone ‚Äî like a helpful campus buddy who knows their stuff (but not everything!).
    - Based on the {source}, always provide citations.
    - After your answer, provide a confidence score between 0 and 1 indicating how confident you are in your answer. Format it exactly as: "[Confidence: 0.85]"

    Previous Chat Summary:
    {chat_summary}
    
    Context:
    {context}
    
    Question:
    {query}
    
    Answer:
    """

    llm = get_llm()

    return llm.invoke(prompt)


def generate_updated_summary(prev_summary, latest_chat_record):
    """
    Generate an updated summary based on the previous summary and the latest chat record.
    """
    latest_chat_record_formatted = "- User: " + latest_chat_record["raw_query"] + "\n- AI: " + latest_chat_record["answer"]

    prompt = f"""
    You are a helpful assistant responsible for summarizing user conversations.

    Here is the previous chat summary:
    {prev_summary}

    Here is the latest user message:
    {latest_chat_record_formatted}

    Generate a new concise and coherent summary that incorporates the latest message into the previous summary.
    """

    llm = get_llm()
    return llm.invoke(prompt).content.strip()