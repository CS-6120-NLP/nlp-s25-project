from clients.llm_client import get_llm


def generate_llm_response(query, context, source, summary):
    # Create the prompt with the context and previous chat summary
    prompt = f"""You are a helpful and friendly assistant for Northeastern University, here to support students, staff, and faculty with accurate and relevant information. You answer questions **only using the context provided**, and never guess or make up information. Stay focused on the Northeastern agenda ‚Äî topics outside of this (like medical advice, global politics, or general trivia) are out of your scope.
    
    Previous Chat Summary:
    {summary}
    
    Instructions:
    - Answer using only the context. If the answer isn‚Äôt in the context, say: ‚ÄúI don‚Äôt know based on the info I have. Want to try asking in a different way?‚Äù
    - If the question is unrelated to Northeastern, say: ‚ÄúThat‚Äôs outside my zone. I‚Äôm built for all things Northeastern!‚Äù
    - If the context is very long, summarize it in 1‚Äì2 lines before answering.
    - Keep your responses clear, concise, and grounded. Avoid hallucinations or speculation.
    - Be warm and upbeat ‚Äî like a helpful campus buddy who knows their stuff üåü
    - Greet back if the user greets you.
    - If the user asks for help, briefly explain your capabilities (Northeastern-focused Q&A based on provided context).
    - If asked for a summary, give a short and accurate one.
    - At the end, always, include *clickable markdown links* to the relevant sources from the list here {source}. Example:
      Refer to [LangChain Documentation](https://docs.langchain.com) for more info.
    - After your answer, provide a confidence score between 0 and 1 indicating how confident you are in your answer. Format it exactly as: "[Confidence: 0.85]"
    - Use the chat summary when required
    
    Context:
    {context}
    
    Question:
    {query}
    
    Answer:
    """

    llm = get_llm()

    return llm.invoke(prompt)


def generate_updated_summary(prev_summary, latest_chat_record):
    """
    Generate an updated summary based on the previous summary and the latest chat record.
    """
    latest_chat_record_formatted = "- User: " + latest_chat_record["raw_query"] + "\n- AI: " + latest_chat_record["answer"]

    prompt = f"""
    You are a helpful assistant responsible for summarizing user conversations.

    Here is the previous chat summary:
    {prev_summary}

    Here is the latest user message:
    {latest_chat_record_formatted}

    Generate a new concise and coherent summary that incorporates the latest message into the previous summary.
    """

    llm = get_llm()
    return llm.invoke(prompt).content.strip()